"""
ä¸‰æ­¥ AI æµæ°´çº¿
Step 1: å¯¹ä»Šæ—¥ RawNews è¿›è¡Œç»´åº¦åˆ†ç±»ï¼ˆpolicy/global/market/tech/consumer/industry/vc/economyï¼‰
Step 2: æ¯ä¸ªç»´åº¦æŒ‘é€‰æœ€ç›¸å…³çš„ 3-5 æ¡ï¼Œç”Ÿæˆå·¨å¤´è§†è§’æ´å¯Ÿ
Step 3: åˆå¹¶æˆå®Œæ•´ Markdown æ ¼å¼çš„æ¯æ—¥å†³ç­–ç®€æŠ¥ï¼Œå¹¶å†™å…¥ DailyReport è¡¨
"""
from __future__ import annotations

import json
import logging
import os
import sys
from datetime import date, datetime, timedelta

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database import SessionLocal
from models import DailyReport, RawNews

logger = logging.getLogger(__name__)

# ç»´åº¦å®šä¹‰
SECTIONS: dict[str, str] = {
    "policy":   "å®è§‚æ”¿ç­–ï¼ˆæ”¿åºœæ–‡ä»¶ã€å¤®è¡Œè´§å¸æ”¿ç­–ã€ç›‘ç®¡åŠ¨æ€ã€è´¢ç¨æ”¹é©ï¼‰",
    "global":   "å›½é™…å½¢åŠ¿ï¼ˆåœ°ç¼˜æ”¿æ²»ã€ä¸­ç¾å…³ç³»ã€å¤–è´¸è¿›å‡ºå£ã€æ±‡ç‡å˜åŠ¨ï¼‰",
    "market":   "èµ„æœ¬å¸‚åœºï¼ˆAè‚¡æ¸¯è‚¡ç¾è‚¡ã€å¤§å®—å•†å“ã€å€ºåˆ¸ã€åŠ å¯†è´§å¸ï¼‰",
    "tech":     "AIä¸ç¡¬ç§‘æŠ€ï¼ˆäººå·¥æ™ºèƒ½ã€èŠ¯ç‰‡ã€æ–°ææ–™ã€é‡å­è®¡ç®—ï¼‰",
    "consumer": "æ¶ˆè´¹ä¸ç¤¾ä¼šæƒ…ç»ªï¼ˆæ¶ˆè´¹è¶‹åŠ¿ã€ç¤¾ä¼šçƒ­ç‚¹ã€èˆ†è®ºåŠ¨å‘ï¼‰",
    "industry": "äº§ä¸šèµ›é“ï¼ˆæ–°èƒ½æºã€ç”Ÿç‰©åŒ»è¯ã€æˆ¿åœ°äº§ã€æ±½è½¦ï¼‰",
    "vc":       "åˆ›æŠ•ç”Ÿæ€ï¼ˆèèµ„å¹¶è´­ã€IPOã€ç‹¬è§’å…½ã€æŠ•èµ„æœºæ„åŠ¨æ€ï¼‰",
    "economy":  "ç»æµæ•°æ®ï¼ˆGDPã€PMIã€CPIã€å°±ä¸šã€å¤–æ±‡å‚¨å¤‡ï¼‰",
}

SECTION_TITLES: dict[str, str] = {
    "policy":   "ğŸ›ï¸ å®è§‚æ”¿ç­–",
    "global":   "ğŸŒ å›½é™…å½¢åŠ¿",
    "market":   "ğŸ“ˆ èµ„æœ¬å¸‚åœº",
    "tech":     "ğŸ¤– AI ä¸ç¡¬ç§‘æŠ€",
    "consumer": "ğŸ›ï¸ æ¶ˆè´¹ä¸ç¤¾ä¼šæƒ…ç»ª",
    "industry": "âš¡ äº§ä¸šèµ›é“",
    "vc":       "ğŸ¦„ åˆ›æŠ•ç”Ÿæ€",
    "economy":  "ğŸ“Š ç»æµæ•°æ®",
}

TITAN_PERSPECTIVES = """
ä»¥ä¸‹æ˜¯ä½ éœ€è¦æ¨¡æ‹Ÿçš„8ä½å·¨å¤´è§†è§’ï¼š
- æå˜‰è¯šï¼ˆç¨³å¥èµ„æœ¬ä¸»ä¹‰ï¼Œç°é‡‘ä¸ºç‹ï¼Œé€†å‘å¸ƒå±€ï¼‰
- å·´è²ç‰¹ï¼ˆä»·å€¼æŠ•èµ„ï¼ŒæŠ¤åŸæ²³ï¼Œé•¿æœŸæŒæœ‰ï¼‰
- æŸ¥ç†Â·èŠ’æ ¼ï¼ˆå¤šå…ƒæ€ç»´æ¨¡å‹ï¼Œé€†å‘æ€è€ƒï¼Œé¿å…è ¢äº‹ï¼‰
- ä»»æ­£éï¼ˆæŠ€æœ¯è‡ªä¸»ï¼Œç»„ç»‡éŸ§æ€§ï¼Œåä¸ºç”Ÿå­˜å“²å­¦ï¼‰
- å¼ ç£Šï¼ˆé•¿æœŸä¸»ä¹‰ï¼Œä¸­å›½æ¶ˆè´¹å‡çº§ï¼Œæ·±åº¦ç ”ç©¶ï¼‰
- é©¬æ–¯å…‹ï¼ˆç¬¬ä¸€æ€§åŸç†ï¼ŒæŒ‡æ•°å¢é•¿ï¼Œé¢ è¦†æ€§åˆ›æ–°ï¼‰
- é»„ä»å‹‹ï¼ˆAIåŸºç¡€è®¾æ–½ï¼Œç®—åŠ›å³æœªæ¥ï¼‰
- é›·å†›ï¼ˆé£å£åˆ¤æ–­ï¼Œç”Ÿæ€æ•´åˆï¼Œæè‡´æ•ˆç‡ï¼‰
"""


def _ai_classify(titles_text: str) -> dict[str, list[int]]:
    """
    Step 1: è°ƒç”¨ AI å°†æ–°é—»åˆ—è¡¨åˆ†ç±»åˆ°å…«å¤§ç»´åº¦ã€‚
    è¿”å› {section: [news_id, ...]}
    """
    try:
        from ai.volcengine import chat
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢ç»ä¿¡æ¯åˆ†ç±»åŠ©æ‰‹ã€‚è¯·å°†ä¸‹é¢çš„æ–°é—»åˆ—è¡¨åˆ†ç±»åˆ°å¯¹åº”çš„ç»´åº¦ä¸­ã€‚

ç»´åº¦è¯´æ˜ï¼š
{json.dumps(SECTIONS, ensure_ascii=False, indent=2)}

æ–°é—»åˆ—è¡¨ï¼ˆæ ¼å¼ï¼šID | æ ‡é¢˜ï¼‰ï¼š
{titles_text}

è¯·è¿”å› JSONï¼Œæ ¼å¼ä¸º {{"policy": [id1, id2, ...], "global": [...], ...}}
åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚å¦‚æœæŸç»´åº¦æ— å¯¹åº”æ–°é—»ï¼Œå€¼ä¸ºç©ºæ•°ç»„ã€‚
"""
        result = chat("ä½ æ˜¯ä¸“ä¸šçš„è´¢ç»ä¿¡æ¯åˆ†ç±»åŠ©æ‰‹ï¼Œç”¨JSONæ ¼å¼å›å¤ã€‚", prompt, temperature=0.2)
        # æå– JSON éƒ¨åˆ†
        start = result.find('{')
        end   = result.rfind('}') + 1
        if start == -1 or end == 0:
            logger.warning("AI åˆ†ç±»æœªè¿”å›æœ‰æ•ˆ JSONï¼Œfallback åˆ°ç©ºåˆ†ç±»")
            return {k: [] for k in SECTIONS}
        return json.loads(result[start:end])
    except Exception as e:
        logger.warning(f"AI åˆ†ç±»å¤±è´¥ï¼ˆå¯èƒ½ API æœªé…ç½®ï¼‰: {e}")
        return {k: [] for k in SECTIONS}


def _ai_generate_brief(section_news: dict[str, list[dict]]) -> tuple[str, int, int]:
    """
    Step 3: ç”Ÿæˆå®Œæ•´ Markdown ç®€æŠ¥ã€‚
    è¿”å› (markdown, macro_score, tech_score)
    """
    try:
        from ai.volcengine import chat

        sections_text = ""
        for sec, items in section_news.items():
            if not items:
                continue
            title = SECTION_TITLES.get(sec, sec)
            sections_text += f"\n\n### {title}\n"
            for it in items[:20]:
                sections_text += f"- {it['title']}\n"

        prompt = f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§å•†ä¸šå†³ç­–é¡¾é—®ï¼Œè¯·æ ¹æ®ä»Šæ—¥æƒ…æŠ¥ç”Ÿæˆæ¯æ—¥æ™¨é—´å†³ç­–ç®€æŠ¥ã€‚

{TITAN_PERSPECTIVES}

ä»Šæ—¥å…³é”®æƒ…æŠ¥ï¼š
{sections_text}

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„ Markdown æ ¼å¼ç®€æŠ¥ï¼ŒåŒ…å«ï¼š
1. **ä»Šæ—¥å¸‚åœºæ¦‚è¿°**ï¼ˆ2-3å¥ï¼‰
2. **å…«å¤§ç»´åº¦è§£è¯»**ï¼ˆæ¯ä¸ªç»´åº¦2-3æ¡è¦ç‚¹ï¼Œç»“åˆå·¨å¤´è§†è§’ï¼‰
3. **ä»Šæ—¥ä¿¡å·ç¯**ï¼š
   - ğŸ”´ é£é™©é¢„è­¦ï¼ˆ2-3æ¡ï¼‰
   - ğŸŸ¢ æœºä¼šä¿¡å·ï¼ˆ2-3æ¡ï¼‰
   - ğŸ”µ ä»·å€¼æ´¼åœ°ï¼ˆ1-2æ¡ï¼‰
4. **å®è§‚è¯„åˆ†** (0-100) å’Œ **ç§‘æŠ€è¯„åˆ†** (0-100)ï¼ˆæœ€åå•ç‹¬ä¸€è¡Œï¼Œæ ¼å¼ï¼šSCORES: macro=XX tech=XXï¼‰

ä¿æŒä¸“ä¸šã€ç®€æ´ï¼Œé¢å‘ä¼ä¸šå®¶å’ŒæŠ•èµ„äººã€‚
"""
        result = chat(
            "ä½ æ˜¯é¡¶çº§å•†ä¸šå†³ç­–é¡¾é—®ï¼Œç”Ÿæˆä¸“ä¸šçš„æŠ•èµ„å†³ç­–ç®€æŠ¥ã€‚",
            prompt,
            temperature=0.6,
        )

        # æå–è¯„åˆ†
        macro_score, tech_score = 70, 70
        for line in result.splitlines():
            if line.strip().startswith("SCORES:"):
                parts = line.replace("SCORES:", "").strip().split()
                for p in parts:
                    if p.startswith("macro="):
                        macro_score = int(p.split("=")[1])
                    elif p.startswith("tech="):
                        tech_score = int(p.split("=")[1])
                result = result.replace(line, "").strip()
                break

        return result, macro_score, tech_score

    except Exception as e:
        logger.warning(f"AI ç”Ÿæˆç®€æŠ¥å¤±è´¥: {e}")
        # Fallbackï¼šç”¨è§„åˆ™ç”Ÿæˆç®€å• Markdown
        sections_md = ""
        for sec, items in section_news.items():
            if not items:
                continue
            title = SECTION_TITLES.get(sec, sec)
            sections_md += f"\n### {title}\n"
            for it in items[:20]:
                sections_md += f"- [{it['title']}]({it.get('url', '#')})\n"

        md = f"# æ¯æ—¥æ™¨é—´å†³ç­–ç®€æŠ¥\n\n> âš ï¸ AI ç®€æŠ¥ç”Ÿæˆå¤±è´¥ï¼ˆAPI æœªé…ç½®ï¼‰ï¼Œä»¥ä¸‹ä¸ºåŸå§‹æƒ…æŠ¥æ±‡æ€»\n\n{sections_md}"
        return md, 70, 70


def run_pipeline(target_date: date | None = None) -> DailyReport:
    """
    æ‰§è¡Œå®Œæ•´çš„ä¸‰æ­¥ AI æµæ°´çº¿ï¼Œç”Ÿæˆå¹¶ä¿å­˜ DailyReportã€‚
    å¦‚æœå½“å¤©ç®€æŠ¥å·²å­˜åœ¨åˆ™å…ˆåˆ é™¤å†é‡å»ºã€‚
    """
    if target_date is None:
        target_date = date.today()

    db = SessionLocal()
    try:
        # åˆ é™¤å·²æœ‰ç®€æŠ¥ï¼ˆå…è®¸é‡è·‘ï¼‰
        existing = db.query(DailyReport).filter(DailyReport.report_date == target_date).first()
        if existing:
            db.delete(existing)
            db.commit()

        # å–æœ€è¿‘24å°æ—¶æ–°é—»
        since = datetime.combine(target_date, datetime.min.time()) - timedelta(hours=6)
        news_items = (
            db.query(RawNews)
            .filter(RawNews.crawl_time >= since)
            .order_by(RawNews.crawl_time.desc())
            .limit(200)
            .all()
        )

        if not news_items:
            logger.warning(f"[Pipeline] {target_date} æ— æ–°é—»ï¼Œç”Ÿæˆç©ºç®€æŠ¥")
            report = DailyReport(
                report_date=target_date,
                summary_markdown="# ä»Šæ—¥æš‚æ— æƒ…æŠ¥\n\nè¯·å…ˆè¿è¡Œçˆ¬è™«ï¼š`python scheduler.py --once`",
                macro_score=None,
                tech_score=None,
            )
            db.add(report)
            db.commit()
            db.refresh(report)
            return report

        logger.info(f"[Pipeline] Step 1: å¯¹ {len(news_items)} æ¡æ–°é—»è¿›è¡Œåˆ†ç±»...")
        titles_text = "\n".join(f"{n.id} | {n.title}" for n in news_items)
        classified   = _ai_classify(titles_text)

        # æ„å»ºåˆ†ç±»æ˜ å°„ {id: news_obj}
        news_map = {n.id: n for n in news_items}

        # å¦‚æœåˆ†ç±»å…¨ä¸ºç©ºï¼ˆAI æœªé…ç½®ï¼‰ï¼ŒæŒ‰æ¥æºå¹³å°åšç®€å•æ˜ å°„
        total_classified = sum(len(v) for v in classified.values())
        if total_classified == 0:
            logger.info("[Pipeline] åˆ†ç±»ç»“æœä¸ºç©ºï¼Œä½¿ç”¨å¹³å°è§„åˆ™åˆ†ç±»")
            PLATFORM_SECTION_MAP = {
                "gov":   "policy", "ndrc": "policy", "stats": "economy",
                "xinhua": "global", "reuters": "global", "sina": "market",
                "stcn": "market", "caixin": "market", "36kr": "vc",
                "hackernews": "tech", "weibo": "consumer", "baidu": "consumer",
            }
            for n in news_items:
                sec = PLATFORM_SECTION_MAP.get(n.source_platform, "global")
                classified.setdefault(sec, []).append(n.id)

            # ç”¨ tags è¦†ç›–
            for n in news_items:
                if n.tags:
                    for t in (n.tags if isinstance(n.tags, list) else []):
                        if t in SECTIONS:
                            for sec in classified.values():
                                if n.id in sec:
                                    sec.remove(n.id)
                            classified.setdefault(t, []).append(n.id)
                            break

        # æ›´æ–° RawNews.tags ä¸ºåˆ†ç±»ç»“æœ
        for sec, ids in classified.items():
            for nid in ids:
                n = news_map.get(nid)
                if n:
                    existing_tags = list(n.tags) if n.tags else []
                    if sec not in existing_tags:
                        existing_tags = [sec] + [t for t in existing_tags if t != sec]
                    n.tags = existing_tags[:5]
        db.commit()

        logger.info("[Pipeline] Step 2: æ„å»ºå„ç»´åº¦æ–°é—»æ‘˜è¦...")
        section_news: dict[str, list[dict]] = {}
        for sec, ids in classified.items():
            section_news[sec] = []
            for nid in ids[:20]:
                n = news_map.get(nid)
                if n:
                    section_news[sec].append({
                        "id": n.id, "title": n.title,
                        "url": n.url or "", "platform": n.source_platform,
                    })

        logger.info("[Pipeline] Step 3: ç”Ÿæˆ AI ç®€æŠ¥...")
        summary_md, macro_score, tech_score = _ai_generate_brief(section_news)

        report = DailyReport(
            report_date=target_date,
            summary_markdown=summary_md,
            macro_score=macro_score,
            tech_score=tech_score,
        )
        db.add(report)
        db.commit()
        db.refresh(report)
        logger.info(f"[Pipeline] ç®€æŠ¥ç”Ÿæˆå®Œæˆï¼Œå®è§‚={macro_score} ç§‘æŠ€={tech_score}")
        return report

    finally:
        db.close()


if __name__ == "__main__":
    import sys
    logging.basicConfig(level=logging.INFO)
    d = date.fromisoformat(sys.argv[1]) if len(sys.argv) > 1 else date.today()
    r = run_pipeline(d)
    print(f"\n=== ç®€æŠ¥ç”Ÿæˆå®Œæˆ ===\næ—¥æœŸ: {r.report_date}\nå®è§‚è¯„åˆ†: {r.macro_score}\nç§‘æŠ€è¯„åˆ†: {r.tech_score}")
    print(f"\n--- ç®€æŠ¥é¢„è§ˆ (å‰500å­—) ---\n{(r.summary_markdown or '')[:500]}...")
